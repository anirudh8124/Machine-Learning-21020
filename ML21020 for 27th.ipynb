{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjzFNrTIJeRU8sfStKGzA8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEJ-wkH4mJfv","executionInfo":{"status":"ok","timestamp":1695117386906,"user_tz":-330,"elapsed":511,"user":{"displayName":"Anirudh S","userId":"17490394436853575963"}},"outputId":"45a259e1-1729-40a7-831b-d9ab6d98b499"},"outputs":[{"output_type":"stream","name":"stdout","text":["Information Gain for Age: 0.24674981977443933\n","Information Gain for Income: 0.02922256565895487\n","Information Gain for Student: 0.15183550136234159\n","Information Gain for Credit_Rating: 0.10224356360985076\n","The first feature to select for constructing the decision tree is: Age\n","The highest information gain is: 0.24674981977443933\n","Training Set Accuracy: 1.0\n","Test Set Accuracy: 1.0\n","The depth of the constructed Decision Tree is: 4\n"]}],"source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.tree import plot_tree\n","\n","# Sample dataset (replace with your project data)\n","data = {\n","    'Age': ['<=30', '<=30', '31-40', '>40', '>40', '>40', '31-40', '<=30', '<=30', '>40', '<=30', '31-40', '31-40', '>40'],\n","    'Income': ['High', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Medium'],\n","    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No'],\n","    'Credit_Rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Fair', 'Fair', 'Excellent'],\n","    'Buys_Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Convert categorical features to numerical using Label Encoding\n","label_encoder = LabelEncoder()\n","\n","for column in df.columns:\n","    df[column] = label_encoder.fit_transform(df[column])\n","\n","# Separate features (X) and target (y)\n","X = df.drop(columns=['Buys_Computer'])\n","y = df['Buys_Computer']\n","\n","# Split the data into training and testing sets\n","Tr_X, Te_X, Tr_y, Te_y = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Function to calculate entropy\n","def calculate_entropy(data):\n","    class_labels = data.unique()\n","    entropy = 0\n","    total_instances = len(data)\n","\n","    for label in class_labels:\n","        p = len(data[data == label]) / total_instances\n","        entropy -= p * np.log2(p)\n","\n","    return entropy\n","\n","# Function to calculate information gain\n","def calculate_information_gain(data, feature, target):\n","    entropy_before_split = calculate_entropy(target)\n","    total_instances = len(data)\n","\n","    weighted_entropy_after_split = 0\n","\n","    for value in data.unique():\n","        subset_indices = data[data == value].index\n","        subset_target = target[subset_indices]\n","        subset_instances = len(subset_indices)\n","        weighted_entropy_after_split += (subset_instances / total_instances) * calculate_entropy(subset_target)\n","\n","    information_gain = entropy_before_split - weighted_entropy_after_split\n","    return information_gain\n","\n","# Calculate entropy and information gain for each feature\n","features = X.columns  # Use the column names of X as features\n","information_gains = {}\n","\n","for feature in features:\n","    information_gains[feature] = calculate_information_gain(X[feature], feature, y)\n","    print(f\"Information Gain for {feature}: {information_gains[feature]}\")\n","\n","# Find the feature with the highest information gain (the root node)\n","root_node = max(information_gains, key=information_gains.get)\n","\n","# Find the highest information gain\n","highestinfo_gain = max(information_gains.values())\n","\n","print(f\"The first feature to select for constructing the decision tree is: {root_node}\")\n","print(f\"The highest information gain is: {highestinfo_gain}\")\n","\n","# Create and fit the Decision Tree model\n","model = DecisionTreeClassifier()\n","model.fit(Tr_X, Tr_y)\n","\n","# Calculate the accuracy on the training set\n","trainaccuracy = model.score(Tr_X, Tr_y)\n","print(f\"Training Set Accuracy: {trainaccuracy}\")\n","\n","# Calculate the accuracy on the test set\n","testaccuracy = model.score(Te_X, Te_y)\n","print(f\"Test Set Accuracy: {testaccuracy}\")\n","\n","# Calculate the depth of the constructed tree\n","treedepth = model.get_depth()\n","print(f\"The depth of the constructed Decision Tree is: {treedepth}\")\n","\n","# Visualize the constructed tree\n","plt.figure(figsize=(20, 10))\n","plot_tree(model, filled=True)\n","plt.show()\n","\n"]}]}